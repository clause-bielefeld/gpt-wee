{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e013210b",
   "metadata": {},
   "source": [
    "# Training GPT-Wee\n",
    "\n",
    "If you want to use this notebook to train your own very small GPT-2 model, you have to adapt all ```/path/to/``` to use your own local path:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4e385",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b20ad",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9437fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer)\n",
    "from tqdm import tqdm \n",
    "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbb4723",
   "metadata": {},
   "source": [
    "Initialize with BPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1208c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e4b9a",
   "metadata": {},
   "source": [
    "Normalizer that sets everything to normal unicode, lowercase, and strips white spaces and accents\n",
    "\n",
    "(explanations here: https://huggingface.co/docs/tokenizers/components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = normalizers.Sequence([NFD(), Lowercase(), Strip(), StripAccents()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ac5e5",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.normalize_str(\"Héllò hôw are ü?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2ea64",
   "metadata": {},
   "source": [
    "Pre-tokenization (division of text into tokens on which BPE can be performed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff60ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=8000, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b920df3",
   "metadata": {},
   "source": [
    "Data from https://github.com/babylm/babylm.github.io/raw/main/babylm_data.zip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9cb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfiles = [\"/path/to/babylm_data/babylm_10M/aochildes.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/bnc_spoken.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/cbt.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/children_stories.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/gutenberg.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/open_subtitles.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/qed.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/simple_wikipedia.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/switchboard.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/wikipedia.train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files = textfiles, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e94458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b82e68",
   "metadata": {},
   "source": [
    "By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don’t want the offsets to include these whitespaces, then this PostProcessor must be used:\n",
    "\n",
    "(https://huggingface.co/docs/tokenizers/api/post-processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f57718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3146c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "start, end = encoding.offsets[4]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3401dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456297ab",
   "metadata": {},
   "source": [
    "Save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer.save_pretrained(\"/path/to/tokenizer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5ef16",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dff5b",
   "metadata": {},
   "source": [
    "Load tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c13c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/path/to/tokenizer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc7af9",
   "metadata": {},
   "source": [
    "#### For regular learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57762c85",
   "metadata": {},
   "source": [
    "Load files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46009dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = [\"/path/to/babylm_data/babylm_10M/aochildes.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/bnc_spoken.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/cbt.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/children_stories.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/gutenberg.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/open_subtitles.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/qed.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/simple_wikipedia.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/switchboard.train\",\n",
    "                 \"/path/to/babylm_data/babylm_10M/wikipedia.train\"]\n",
    "\n",
    "eval_files = [\"/path/to/babylm_data/babylm_dev/aochildes.dev\",\n",
    "             \"/path/to/babylm_data/babylm_dev/bnc_spoken.dev\",\n",
    "             \"/path/to/babylm_data/babylm_dev/cbt.dev\",\n",
    "             \"/path/to/babylm_data/babylm_dev/children_stories.dev\",\n",
    "             \"/path/to/babylm_data/babylm_dev/gutenberg.dev\",\n",
    "             \"/path/to/babylm_data/babylm_dev/open_subtitles.dev\",\n",
    "             \"//path/to//babylm_data/babylm_dev/qed.dev\",\n",
    "             \"/path/to//babylm_data/babylm_dev/simple_wikipedia.dev\",\n",
    "             \"/path/to/babylm_data/babylm_dev/switchboard.dev\",\n",
    "             \"/path/to/babylm_data/babylm_dev/wikipedia.dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('text', data_files={'train': training_files, \n",
    "                                           'validation': eval_files})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cac640",
   "metadata": {},
   "source": [
    "#### For curriculum learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b60c79",
   "metadata": {},
   "source": [
    "Load training data in ```streaming```-mode, so that it gets loaded progressively (quick and dirty implementation of curriculum ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = [\"/path/to/babylm-curriculum/ordered_text.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e165e6",
   "metadata": {},
   "source": [
    "Ordered text from ```sentence_scoring.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d06e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"text\", data_files={\"train\": training_files, \n",
    "                                           \"validation\": eval_files}, streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebd899",
   "metadata": {},
   "source": [
    "Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39eace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        #if length == context_length:\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad620bca",
   "metadata": {},
   "source": [
    "Initiate new model (and specify model architecture according to https://huggingface.co/docs/transformers/v4.30.0/en/model_doc/gpt2#transformers.GPT2Config):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    n_embd = 128,\n",
    "    n_layer = 2,\n",
    "    n_head = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee13f98",
   "metadata": {},
   "source": [
    "Show how many parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac85743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f2677",
   "metadata": {},
   "source": [
    "Training arguments (https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) can be optimized here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"toy_model_outputs\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    #use_mps_device=True, # enable when training on Mac with Apple Silicon\n",
    "    max_steps = 44000 # enable for curriculum learning, disable for normal\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],#[:8000]['input_ids'],\n",
    "    eval_dataset=tokenized_datasets['validation'],#[:2000]['input_ids'],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c964ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4523d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/path/t/babyGPTs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(trainer.state.log_history).to_csv(\"/path/to/stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model='/path/to/babyGPTs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a921e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"The lady\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(txt, num_return_sequences=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e29499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
